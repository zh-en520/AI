# 机器学习DAY01

## 机器学习

### 概述

#### 什么是机器学习

机器学习是一门能够让编程计算机从数据中学习的计算机科学。

一个计算机程序在完成任务T之后，获得经验E，其表现效果为P。如果任务T的性能表现（P），虽则E增加而增加，那么这样的计算机程序就被称为机器学习系统。自我完善、自我增进、自我适应。

#### 为什么需要机器学习

* 自动化升级维护
* 解决那些算法太复杂甚至根本没有算法实现的问题
* 在机器学习过程中协助人类看世界

#### 机器学习的问题

1. 建模问题

   在数据对象中通过统计或推理的方法，寻找一个接收特定输入x，并给出预期输出y的功能函数f。

2. 评估问题

   针对 已知的输入，函数给出的输出与实际输出之间一定存在误差，因此需要构建一个评估体系，根据误差大小判断模型的优劣。

3. 优化问题

   学习的核心在于改善性能，通过对算法的反复迭代，不断提升函数预测的准确性。

#### 机器学习的种类

**监督学习、无监督学习、半监督学习、强化学习**

1. 监督学习：用已知的输入与输出训练模型，并评估模型的性能。
2. 无监督学习：在没有已知输出的情况下，仅仅根据输入的相关性，进行类别的划分。
3. 半监督学习：先通过无监督学习划分类别，再根据人工标记通过有监督学习预测输出。
4. 强化学习：通过对不同决策结果的奖励和惩罚，使机器学习系统在经过足够长时间的训练以后，越来越倾向于给出接近期望结果的输出。

**批量学习和增量学习**

1. 批量学习：将学习的过程和应用的过程截然分开，用全部的训练数据训练模型，然后在应用场景中实现预测。当预测结果不够理想时，重新回到学习过程，如此循环。

2. 增量学习：将学习的过程与应用的过程统一起来，在应用的同时以增量的方式，不断学习新的内容，边训练边预测。

**基于实例的学习、基于模型的学习**

1. 根据以往的经验，寻找与待预测输入最接近的样本，以其输出作为预测结果。

| 年龄 | 学历 | 经验 | 性别 | 月薪  |
| ---- | ---- | ---- | ---- | ----- |
| 25   | 硕士 | 2    | 女   | 10000 |
| 23   | 本科 | 3    | 男   | 9000  |
| 20   | 本科 | 3    | 男   | ?     |

2. 跟与以往的输入与输出建立用于联系输入与输出的某种数学模型，将待预测输入代入该模型，预测其结果。

#### 机器学习程序的一般思路

1. 数据处理   

   数据收集（数据检索，数据挖掘，爬虫）

   数据清洗（python及大数据数据预处理）

2. 机器学习

   选择模型（选择算法）

   训练模型（算法）

   评估模型（工具、框架、算法）

   测试模型

3. 业务运维

   应用模型

   维护模型

#### 机器学习的典型应用

事件预测、推荐引擎、自然语言识别、语音识别、图像识别、人脸识别等。

#### 机器学习的基本问题

1. 回归问题：根据已知的输入与输出，寻找某种性能最佳的模型，将未知输出的输入带入模型，得到连续的输出。
2. 分类问题：根据已知的输入与输出，寻找某种性能最佳的模型，将未知输出的输入带入模型，得到离散的输出。
3. 聚类问题：根据已知输入的相似程度，将其划分为不同的群落。
4. 降维问题：在性能损失尽可能小的前提下，降低数据的复杂度。

### 数据预处理

普及知识：sklearn所能处理的样本数据结构：

| 年龄 | 学历 | 经验 | 性别 | 月薪  |
| ---- | ---- | ---- | ---- | ----- |
| 25   | 硕士 | 2    | 女   | 10000 |
| 23   | 本科 | 3    | 男   | 9000  |
| 20   | 本科 | 3    | 男   | ?     |

一行一样本， 一列一特征。

**数据预处理相关：**

```python
import sklearn.preprocessing as sp
```

#### 均值移除（标准化）

由于一个样本的不同特征值差异较大， 不利于使用现有机器学习算法进行样本处理。均值移除可以让样本矩阵中每一列的平均值为0，标准差为1.

如何使一组数据的平均值为0？

```
S = [17  20  23]
mean = 20
S-mean -> [-3  0  3]
```

如何使一组数据的标准差为1？

```
S = [-3  0  3]
std = np.std(S)
S2 = S / std -> [-3/std, 0/std, 3/std]
```

均值移除相关API：

```python
# array为原样本矩阵，返回均值移除后的结果
A = sp.scale(array)
```

案例：

```python
"""
demo02_scale.py 均值移除
"""
import sklearn.preprocessing as sp
import numpy as np

raw_samples = np.array([
	[17, 100, 4000],
	[20, 80, 5000],
	[23, 75, 3500]])
A = sp.scale(raw_samples)
print(A)
print(A.mean(axis=0))  #每列的均值
print(A.std(axis=0))  #每列的标准差
```

#### 范围缩放

将样本矩阵中每一列的最小值和最大值设定为相同的区间，统一各列特征值的范围。一般都统一在[0,1]区间。

如何使一组数据最小值为0？

```python
S=[17, 20, 30]
S-17 ->  [0, 3, 6]
```

如何使一组数据最大值为1？

```python
S = [0, 3, 6]
S / 6 -> [0, 0.5, 1]
```

相关API：

```python
# 范围缩放器
m=sp.MinMaxScaler(feature_range=(0,1))
result=m.fit_transform(原始样本矩阵)
```

案例：

```python
"""
demo03_minmaxscaler.py 范围缩放
"""
import sklearn.preprocessing as sp
import numpy as np

raw_samples = np.array([
	[17, 100, 4000],
	[20, 80, 5000],
	[23, 75, 3500]])

mms = sp.MinMaxScaler(feature_range=(0,1))
r = mms.fit_transform(raw_samples)
print(r)
```

#### 归一化

有些情况下样本的每个特征具体的值并不重要，但是每个样本特征值在样本所有特征值中的占比更加重要，需要做行级的归一化。

|      | python | java | php  |
| ---- | ------ | ---- | ---- |
| 2017 | 10     | 20   | 5    |
| 2018 | 8      | 5    | 0    |

所以归一化可以用样本的每个特征值除以该样本所有特征值绝对值的总和。这样变换后的样本矩阵，每个样本的特征值绝对值之和为1.

相关API：

```python
# array：原始样本矩阵
# norm：范数
#    l1:  向量中分量绝对值之和为1
#    l2:  向量中分量平方和为1
r = sp.normalize(array, norm='l1')
```

案例：

```python
"""
demo04_normalize.py 归一化
"""
import sklearn.preprocessing as sp
import numpy as np

raw_samples = np.array([
	[17, 100, 4000],
	[20, 80, 5000],
	[23, 75, 3500]])

r = sp.normalize(raw_samples, norm='l1')
print(r)
print(abs(r).sum(axis=1))
```

#### 二值化

二值化可以根据一个事先给定的阈值，用0和1表示特征值不高于或高于阈值。二值化后的样本矩阵中的元素非0即1.达到简化数学模型的目的。

有些业务并不需要分析矩阵的详细的完整数据（图像处理），适合先做二值化再进行模型训练。

相关API：

```python
# 二值化器
bin = sp.Binarizer(threshold=阈值)
r = bin.transform(原始样本矩阵)
```

案例：

```python
"""
demo05_binarizer.py 二值化
"""
import sklearn.preprocessing as sp
import numpy as np

raw_samples = np.array([
	[17, 100, 4000],
	[20, 80, 5000],
	[23, 75, 3500]])

bin = sp.Binarizer(threshold=80)
r = bin.transform(raw_samples)
print(r)
```

#### 独热编码onehot

为样本特征的每个值建立一个由一个1和若干个0组成的序列，用该序列对所有的特征值进行编码。将会生成新的样本矩阵。

```
1		3		2
7		5		4
1		8		6
7		3		9
为每一个数字进行独热编码：
1-10	3-100	2-1000
7-01	5-010	4-0100
		8-001	6-0010
				9-0001
独热编码完毕后得到最终的样本矩阵：
101001000
010100100
100010010
011000001
```

独热编码相关API：

```python
# 创建onehot编码器
# sparse: 是否使用稀疏矩阵
ohe = sp.OneHotEncoder(
    sparse=True, dtype=数据类型)
r = ohe.fit_transform(原始样本矩阵)
```

案例：

```python
"""
demo06_onehot.py  独热编码
"""
import numpy as np
import sklearn.preprocessing as sp

raw_samples = np.array([[1, 3, 2], 
						[7, 5, 4], 
						[1, 8, 6], 
						[7, 3, 9]])
# 创建独热编码器
ohe=sp.OneHotEncoder(sparse=False,dtype=int)
# fit方法意味着：训练后得到编码码表
ohe_dict = ohe.fit(raw_samples)
r = ohe_dict.transform(raw_samples)
print(r)
```

#### 标签编码

由于机器学习模型处理的数据都需要是数字，所以需要把样本数据集中的字符串编码为对应的数字才可以进行模型训练。

相关API：

```python
# 标签编码器
lbe = sp.LabelEncoder()
# 返回编码过后的特征向量
r = lbe.fit_transform(原始特征向量)
# 把数字向量再转回原始字符串向量
samples = lbe.inverse_transform(r)
```

案例：

```python
"""
demo07_lbe.py 标签编码
"""
import numpy as np
import sklearn.preprocessing as sp

raw_samples=np.array(['audi', 'ford',
	'audi', 'toyota', 'ford', 'bmw',
	'toyota', 'ford', 'audi'])

# 执行标签编码
lbe = sp.LabelEncoder()
r = lbe.fit_transform(raw_samples)
print(r)
# 通过数字向量获取对应的字符串向量
ir = lbe.inverse_transform([0,1,1])
print(ir)
```

### 回归模型

#### 线性回归

```
输入	输出
0.5	  5.0
0.6	  5.5
0.8	  6.0
1.1	  6.8
1.4	  7.0
...
```

预测函数： y=w<sub>0</sub> + w<sub>1</sub>x

x：输入

y：输出

w<sub>0</sub> + w<sub>1</sub>：模型参数

所谓的模型训练，就是根据已知的x与y，找到最佳的模型参数w<sub>0</sub> + w<sub>1</sub>，使得尽可能精确的描述输入与输出的关系。

**单样本误差**：

根据预测函数求出输入为x时的预测值：y'=w<sub>0</sub>+w<sub>1</sub>x，单样本误差： 1/2(y'-y)<sup>2</sup>

**总样本误差：**

把所有单样本误差相加：1/2 &Sigma;(y'-y)<sup>2</sup>

**损失函数：**

loss = 1/2 &Sigma;(w<sub>0</sub> + w<sub>1</sub>x - y)<sup>2</sup>

所以损失函数就是总样本误差关于w<sub>0</sub> + w<sub>1</sub>两个模型参数的函数， 该函数属于三维数学模型，可以基于梯度下降算法求取loss函数的极小值。

基于梯度下降理论实现线性回归的步骤：

1. 推导出损失函数
2. 求出w<sub>0</sub> + w<sub>1</sub> 两个方向上的偏导函数，自定义学习率参数使得每次梯度下降迭代时w<sub>0</sub> + w<sub>1</sub> 都在向目标位置进行移动。

3. 不断执行梯度下降，直到找到误差改变量极小（loss函数的极小值）。

案例：基于梯度下降理论实现线性回归绘制回归线。

```python

```











